{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from decimal import Decimal\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from decimal import Decimal\n",
    "from scipy.misc import comb\n",
    "import glob\n",
    "import pickle\n",
    "from operator import itemgetter \n",
    "import itertools\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def load_data(filename):\n",
    "#     data = np.loadtxt(filename,dtype='string')\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unfold_list(temp,x):\n",
    "    for t in temp:\n",
    "        if isinstance(t,list):\n",
    "            unfold_list(t,x)\n",
    "        else:\n",
    "            x.append(t)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(dirname,n):\n",
    "    tokens = []\n",
    "    files = glob.glob(dirname)\n",
    "    \n",
    "    if n:\n",
    "        files_list = files[:n]\n",
    "    else:\n",
    "        files_list = files\n",
    "    for f in files_list:\n",
    "        data = open(f)\n",
    "        temp = []\n",
    "        for line in data:\n",
    "            line = re.sub('\\d+','',line)\n",
    "            temp.append(re.findall(r'\\w+|[!?#$]',line.lower()))\n",
    "        \n",
    "        temp = unfold_list(temp,[])        \n",
    "        c = Counter(temp)\n",
    "        for word in c.keys():\n",
    "            total_word_count = sum(c.values())\n",
    "            c[word] /= 1.*total_word_count\n",
    "        tokens.append(c)\n",
    "    \n",
    "    return tokens        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pickle_files(datastruct,filename,load,dump):\n",
    "    if load:\n",
    "        data = pickle.load(open(filename,'rb'))\n",
    "        print 'end of loading file'\n",
    "        return data\n",
    "    if dump:\n",
    "        pickle.dump(datastruct,open(filename,'wb'))\n",
    "        print 'end of writing file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vocab_doc_freq(list_of_docs):\n",
    "    print 'get_vocab_doc_freq'\n",
    "    tokens = []\n",
    "    for doc in list_of_docs:\n",
    "        for token in doc.keys():\n",
    "            if token not in tokens:\n",
    "                tokens.append(token)\n",
    "    \n",
    "            \n",
    "    doc_freq = defaultdict(int)\n",
    "    for i,t in enumerate(tokens):\n",
    "        for doc in list_of_docs:\n",
    "            if t in doc.keys():\n",
    "                doc_freq[t] += 1\n",
    "     \n",
    "    vocab = [feature for feature in doc_freq if doc_freq[feature] >= 5]\n",
    "#     freq = [(feature,doc_freq[feature]) for feature in vocab]\n",
    "    \n",
    "#     print vocab\n",
    "    if len(vocab) > 3000:\n",
    "        return vocab[:3000]\n",
    "    else:\n",
    "        return vocab\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_vocab_doc_freq\n"
     ]
    }
   ],
   "source": [
    "spam_files = tokenize('spam/*',None)\n",
    "pickle_files(spam_files,'spam_tokens',load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_files = tokenize('ham/*',None)\n",
    "pickle_files(ham_files,'ham_tokens',load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab = get_vocab_doc_freq(spam_files+ham_files)\n",
    "# pickle_files(vocab,'vocabulary',load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of loading file\n",
      "end of loading file\n"
     ]
    }
   ],
   "source": [
    "spam = pickle_files(None,'spam_tokens',load=True,dump=False)\n",
    "ham = pickle_files(None,'ham_tokens',load=True,dump=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print ham[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_vocab_doc_freq\n"
     ]
    }
   ],
   "source": [
    "vocabulary = get_vocab_doc_freq(spam+ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "pickle_files(vocabulary,'vocabulary',load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of loading file\n"
     ]
    }
   ],
   "source": [
    "voc = pickle_files(None,'vocabulary',load=True,dump=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_likelihood(filename):\n",
    "    print 'saving likelihood'\n",
    "    files = spam + ham\n",
    "#     vocab = pickle_files(None,'vocabulary',load=True,dump=False)\n",
    "    X = create_vector(files,voc)\n",
    "    Y = [0]*len(ham) + [1]*len(spam)\n",
    "    labels = [0,1]\n",
    "    mean = dict()\n",
    "    sigma = dict()\n",
    "    likelihood = []\n",
    "    \n",
    "    for l in labels:\n",
    "        mean[l] = calc_mean(X,Y,l)\n",
    "        sigma[l] = calc_std_dev(X,Y,l)\n",
    "    \n",
    "    for i,x in enumerate(X):\n",
    "        likelihood.append((likelihood_function(x,mean[0],sigma[0],voc),likelihood_function(x,mean[1],sigma[1],voc)))\n",
    "        \n",
    "    pickle_files(likelihood,filename,load=False,dump=True)\n",
    "    print 'end of pickling likelihood values'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving likelihood\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function\n",
      "likelihood_function"
     ]
    }
   ],
   "source": [
    "save_likelihood('likelihood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vector(files,vocab):\n",
    "    X = []\n",
    "    for f in files:\n",
    "        temp = [0]*len(vocab)\n",
    "        for i,feature in enumerate(vocab):\n",
    "            if feature in f.keys():\n",
    "                temp[i] = f[feature]\n",
    "        X.append(temp)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indicator(Y,class_val):\n",
    "    l = list()\n",
    "    for idx,y in enumerate(Y):\n",
    "        if y == class_val:\n",
    "            l.append(idx)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_prior(Y,class_val):\n",
    "    m = len(Y)\n",
    "    indices = indicator(Y,class_val)\n",
    "    return (1.*len(indices)/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_mean(X,Y,label):\n",
    "    indices = indicator(Y,label)\n",
    "    x = X[indices]\n",
    "    return np.mean(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_std_dev(X,Y,label):\n",
    "    indices = indicator(Y,label)\n",
    "    x = X[indices]\n",
    "    return np.std(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def likelihood_function(x,mean,sigma,vocab):\n",
    "    print 'likelihood_function'\n",
    "    p = 0.0\n",
    "    for i,xi in enumerate(x):\n",
    "        if xi in vocab:\n",
    "            c = (1/(sigma[i]*math.sqrt(2*math.pi)))\n",
    "            a = ((xi-mean[i])**2)/(2*(sigma[i]**2))\n",
    "            p += math.log(c*math.exp(-a))\n",
    "    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_y(X,Y,labels,x_predict,prior,vocab):\n",
    "    print 'predict_y'\n",
    "    mean = {}\n",
    "    sigma = {}\n",
    "    y = []\n",
    "   \n",
    "    for l in labels:\n",
    "        mean[l] = calc_mean(X,Y,l)\n",
    "        sigma[l] = calc_std_dev(X,Y,l)\n",
    "    \n",
    "\n",
    "    for ind,x in enumerate(x_predict):\n",
    "        temp = []\n",
    "        for l in labels:\n",
    "            a = (likelihood_function(x,mean[l],sigma[l],vocab))\n",
    "            p = (a) + math.log(prior[l])\n",
    "            temp.append(p)\n",
    "        prob = 1.*temp[1]/sum(temp)\n",
    "#         print prob\n",
    "        \n",
    "        if prob > 0.5:\n",
    "            y.append(1)\n",
    "        if prob < 0.5:\n",
    "            y.append(0)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation(dirname,k,verbose):\n",
    "\n",
    "#     spam_files = tokenize('dirname/spam/*',None)\n",
    "#     ham_files = tokenize('dirname/ham/*',None)\n",
    "    \n",
    "#     spam_tokens = pickle_files(None,'spam_tokens',load=True,dump=False)\n",
    "#     ham_tokens = pickle_files(None,'ham_tokens',load=True,dump=False)\n",
    "    files = spam + ham\n",
    "    labels = [1]*len(spam) + [0]*len(ham)\n",
    "    voc = pickle_files(None,'vocabulary',load=True,dump=False)\n",
    "    \n",
    "    classes = [0,1]\n",
    "    \n",
    "    \n",
    "    accuracy = list()\n",
    "    precision = list()\n",
    "    recall = list()\n",
    "    f_measure = list()\n",
    "    prior = dict()\n",
    "    fold = 1\n",
    "    \n",
    "    for train_ind, test_ind in KFold(len(files),k,shuffle=True,random_state=5):\n",
    "         \n",
    "        train_files = itemgetter(*train_ind)(files)\n",
    "        test_files = itemgetter(*test_ind)(files)\n",
    "        train_labels = itemgetter(*train_ind)(labels)\n",
    "        test_labels = itemgetter(*test_ind)(labels)\n",
    "        \n",
    "#         vocab = get_vocab_doc_freq(train_files)\n",
    "        \n",
    "        X_train = create_vector(train_files,voc)\n",
    "        Y_train = np.array(train_labels)\n",
    "        X_test = create_vector(test_files,voc)\n",
    "        Y_test = np.array(test_labels)\n",
    "        \n",
    "        for l in classes:\n",
    "            prior[l] = compute_prior(Y_train,l)\n",
    "            \n",
    "       \n",
    "        Y_predict = predict_y(X_train,Y_train,classes, X_test,prior,voc)\n",
    "        \n",
    "        \n",
    "        acc = accuracy_score(Y_test,Y_predict,normalize=True, sample_weight=None)\n",
    "        c_matrix = confusion_matrix(Y_test, Y_predict)\n",
    "        prec = precision_score(Y_test, Y_predict) \n",
    "        rec = recall_score(Y_test, Y_predict)  \n",
    "        fm = f1_score(Y_test, Y_predict)\n",
    "    \n",
    "        accuracy.append(acc)\n",
    "        recall.append(rec)\n",
    "        precision.append(prec)\n",
    "        f_measure.append(fm)\n",
    "        \n",
    "        if verbose:\n",
    "            print 'fold:', fold\n",
    "            print 'accuracy:', acc\n",
    "            print 'confusion_matrix'\n",
    "            print c_matrix\n",
    "            print 'prediction', prec\n",
    "            print 'recall' , rec\n",
    "            print 'f-measure',fm\n",
    "            print ''\n",
    "        fold += 1\n",
    "    \n",
    "    print accuracy\n",
    "    print precision\n",
    "    print recall\n",
    "    print f_measure\n",
    "    print ' '\n",
    "    \n",
    "    avg_acc = sum(accuracy)/len(accuracy)\n",
    "    avg_pre = sum(precision)/len(precision)\n",
    "    avg_rec = sum(recall)/len(recall)\n",
    "    avg_fm = sum(f_measure)/len(f_measure)\n",
    "   \n",
    "    print 'avg_accuracy: ' , avg_acc\n",
    "    print 'avg_precision', avg_pre\n",
    "    print 'avg_recall', avg_rec\n",
    "    print 'avg_fmeasure', avg_fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features 517\n",
      "prior"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: overflow encountered in double_scalars\n",
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 1\n",
      "accuracy: 0.85\n",
      "confusion_matrix\n",
      "[[10  0]\n",
      " [ 3  7]]\n",
      "prediction 1.0\n",
      "recall 0.7\n",
      "f-measure 0.823529411765\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 2\n",
      "accuracy: 0.9\n",
      "confusion_matrix\n",
      "[[12  0]\n",
      " [ 2  6]]\n",
      "prediction 1.0\n",
      "recall 0.75\n",
      "f-measure 0.857142857143\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 3\n",
      "accuracy: 0.65\n",
      "confusion_matrix\n",
      "[[8 0]\n",
      " [7 5]]\n",
      "prediction 1.0\n",
      "recall 0.416666666667\n",
      "f-measure 0.588235294118\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 4\n",
      "accuracy: 0.55\n",
      "confusion_matrix\n",
      "[[11  0]\n",
      " [ 9  0]]\n",
      "prediction 0.0\n",
      "recall 0.0\n",
      "f-measure 0.0\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 5\n",
      "accuracy: 0.9\n",
      "confusion_matrix\n",
      "[[11  0]\n",
      " [ 2  7]]\n",
      "prediction 1.0\n",
      "recall 0.777777777778\n",
      "f-measure 0.875\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n",
      "\n",
      "fold: 6\n",
      "accuracy: 0.75\n",
      "confusion_matrix\n",
      "[[13  1]\n",
      " [ 4  2]]\n",
      "prediction 0.666666666667\n",
      "recall 0.333333333333\n",
      "f-measure 0.444444444444\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n",
      "[0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 7\n",
      "accuracy: 0.55\n",
      "confusion_matrix\n",
      "[[8 0]\n",
      " [9 3]]\n",
      "prediction 1.0\n",
      "recall 0.25\n",
      "f-measure 0.4\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 8\n",
      "accuracy: 0.65\n",
      "confusion_matrix\n",
      "[[9 1]\n",
      " [6 4]]\n",
      "prediction 0.8\n",
      "recall 0.4\n",
      "f-measure 0.533333333333\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "[0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 9\n",
      "accuracy: 0.8\n",
      "confusion_matrix\n",
      "[[7 0]\n",
      " [4 9]]\n",
      "prediction 1.0\n",
      "recall 0.692307692308\n",
      "f-measure 0.818181818182\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 10\n",
      "accuracy: 0.85\n",
      "confusion_matrix\n",
      "[[9 0]\n",
      " [3 8]]\n",
      "prediction 1.0\n",
      "recall 0.727272727273\n",
      "f-measure 0.842105263158\n",
      "\n",
      "[0.84999999999999998, 0.90000000000000002, 0.65000000000000002, 0.55000000000000004, 0.90000000000000002, 0.75, 0.55000000000000004, 0.65000000000000002, 0.80000000000000004, 0.84999999999999998]\n",
      "[1.0, 1.0, 1.0, 0.0, 1.0, 0.66666666666666663, 1.0, 0.80000000000000004, 1.0, 1.0]\n",
      "[0.69999999999999996, 0.75, 0.41666666666666669, 0.0, 0.77777777777777779, 0.33333333333333331, 0.25, 0.40000000000000002, 0.69230769230769229, 0.72727272727272729]\n",
      "[0.82352941176470584, 0.8571428571428571, 0.58823529411764708, 0.0, 0.87500000000000011, 0.44444444444444442, 0.40000000000000002, 0.53333333333333333, 0.81818181818181812, 0.8421052631578948]\n",
      " \n",
      "avg_accuracy:  0.745\n",
      "avg_precision 0.846666666667\n",
      "avg_recall 0.504735819736\n",
      "avg_fmeasure 0.618197242214\n"
     ]
    }
   ],
   "source": [
    "cross_validation(k=10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features 517\n",
      "prior"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:10: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: overflow encountered in double_scalars\n",
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:12: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 1\n",
      "accuracy: 0.52\n",
      "confusion_matrix\n",
      "[[52  0]\n",
      " [48  0]]\n",
      "prediction 0.0\n",
      "recall 0.0\n",
      "f-measure 0.0\n",
      "\n",
      "prior {0: 0.5, 1: 0.5}\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "fold: 2\n",
      "accuracy: 0.65\n",
      "confusion_matrix\n",
      "[[47  1]\n",
      " [34 18]]\n",
      "prediction 0.947368421053\n",
      "recall 0.346153846154\n",
      "f-measure 0.507042253521\n",
      "\n",
      "[0.52000000000000002, 0.65000000000000002]\n",
      "[0.0, 0.94736842105263153]\n",
      "[0.0, 0.34615384615384615]\n",
      "[0.0, 0.50704225352112675]\n",
      " \n",
      "avg_accuracy:  0.585\n",
      "avg_precision 0.473684210526\n",
      "avg_recall 0.173076923077\n",
      "avg_fmeasure 0.253521126761\n"
     ]
    }
   ],
   "source": [
    "cross_validation(k=2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_validation('data',k=10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
