{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from decimal import Decimal\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from decimal import Decimal\n",
    "from scipy.misc import comb\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "from operator import itemgetter \n",
    "import itertools\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unfold_list(temp,x):\n",
    "    for t in temp:\n",
    "        if isinstance(t,list):\n",
    "            unfold_list(t,x)\n",
    "        else:\n",
    "            x.append(t)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(dirname,n):\n",
    "    tokens = []\n",
    "    files = glob.glob(dirname)\n",
    "    \n",
    "    if n:\n",
    "        files_list = files[:n]\n",
    "    else:\n",
    "        files_list = files\n",
    "    for f in files_list:\n",
    "        data = open(f,mode='r')\n",
    "        temp = []\n",
    "        for line in data:\n",
    "            line = re.sub('\\d+','',line)\n",
    "            temp.append(re.findall(r'\\w+|[!?#$]',line.lower()))\n",
    "        \n",
    "        temp = unfold_list(temp,[])        \n",
    "        c = Counter(temp)\n",
    "#         for word in c.keys():\n",
    "#             total_word_count = sum(c.values())\n",
    "#             c[word] /= 1.*total_word_count\n",
    "        tokens.append(c)\n",
    "    \n",
    "    return tokens        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickle_files(datastruct,filename,load,dump):\n",
    "    \n",
    "    if load:\n",
    "        data = pickle.load(open(filename,mode='r'))\n",
    "        print ('end of loading file')\n",
    "        return data\n",
    "    if dump:\n",
    "        pickle.dump(datastruct,open(filename,'wb'))\n",
    "        print ('end of writing file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab_doc_freq(list_of_docs):\n",
    "    tokens = []\n",
    "    print \"making vocabulary\"\n",
    "    for doc in list_of_docs:\n",
    "        for token in doc.keys():\n",
    "            if token not in tokens:\n",
    "                tokens.append(token)\n",
    "    \n",
    "    doc_freq = defaultdict(int)\n",
    "    for i,t in enumerate(tokens):\n",
    "        for doc in list_of_docs:\n",
    "            if t in doc.keys():\n",
    "                doc_freq[t] += 1\n",
    "     \n",
    "    vocab = [feature for feature in doc_freq if doc_freq[feature] >= 5]\n",
    "    \n",
    "#     if len(vocab) > 3000:\n",
    "#         return vocab[:3000]\n",
    "#     else:\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(files,vocab):\n",
    "    X = []\n",
    "    for f in files:\n",
    "        temp = [0]*len(vocab)\n",
    "        for i,feature in enumerate(vocab):\n",
    "            if feature in f.keys():\n",
    "                temp[i] = f[feature]\n",
    "        X.append(temp)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indicator(Y,class_val):\n",
    "    l = list()\n",
    "    for idx,y in enumerate(Y):\n",
    "        if y == class_val:\n",
    "            l.append(idx)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_prior(Y,class_val):\n",
    "    m = len(Y)\n",
    "    indices = indicator(Y,class_val)\n",
    "    return (1.*len(indices)/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_mean(X,Y,label):\n",
    "    indices = indicator(Y,label)\n",
    "    x = X[indices]\n",
    "    return np.mean(x,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_std_dev(X,Y,label):\n",
    "    indices = indicator(Y,label)\n",
    "    x = X[indices]\n",
    "    return np.std(x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood_function(x,mean,sigma,vocab):\n",
    "    p = 0.0\n",
    "    for i,xi in enumerate(x):\n",
    "        if xi != 0.0 or sigma[i] != 0.0:\n",
    "            c = (1/(sigma[i]*math.sqrt(2*math.pi)))\n",
    "            a = ((xi-mean[i])**2)/(2*(sigma[i]**2))\n",
    "            b = c*math.exp(-a)\n",
    "            if b>0.0:\n",
    "                p += math.log(b)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_y(X,Y,labels,x_predict,prior,vocab):\n",
    "    print ('predict_y')\n",
    "    mean = {}\n",
    "    sigma = {}\n",
    "    y = []\n",
    "   \n",
    "    for l in labels:\n",
    "        mean[l] = calc_mean(X,Y,l)\n",
    "        sigma[l] = calc_std_dev(X,Y,l)\n",
    "    \n",
    "\n",
    "    for ind,x in enumerate(x_predict):\n",
    "        temp = []\n",
    "        for l in labels:\n",
    "            a = (likelihood_function(x,mean[l],sigma[l],vocab))\n",
    "#            print (a)\n",
    "#            print (math.log(prior[l]))\n",
    "            p = (a) + math.log(prior[l])\n",
    "            temp.append(p)\n",
    "#        print (temp)\n",
    "        prob = 1.*temp[1]/sum(temp)\n",
    "        if prob > 0.5:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(0)\n",
    "\n",
    "    print (x_predict.shape)\n",
    "    print (len(y))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(dirname,spam,ham,k,verbose):\n",
    "\n",
    "#    spam = pickle_files(None,'spam_tokens',load=True,dump=False)\n",
    "#    ham = pickle_files(None,'ham_tokens',load=True,dump=False)\n",
    "#    voc = pickle_files(None,'vocabulary',load=True,dump=False)\n",
    "#    files = spam + ham\n",
    "#    labels = [1]*len(spam) + [0]*len(ham)\n",
    "#    voc = pickle_files(None,'vocabulary',load=True,dump=False)\n",
    "\n",
    "#     spam = tokenize(dirname+\"spam/*\",None)\n",
    "#     ham = tokenize(dirname+\"ham/*\",None)\n",
    "    files = spam+ham\n",
    "    labels = [1]*len(spam) + [0]*len(ham)\n",
    "\n",
    "    classes = [0,1]\n",
    "    \n",
    "    accuracy = list()\n",
    "    precision = list()\n",
    "    recall = list()\n",
    "    f_measure = list()\n",
    "    prior = dict()\n",
    "    fold = 1\n",
    "    \n",
    "    for train_ind, test_ind in KFold(len(files),k,shuffle=True,random_state=5):\n",
    "         \n",
    "        train_files = itemgetter(*train_ind)(files)\n",
    "        test_files = itemgetter(*test_ind)(files)\n",
    "        train_labels = itemgetter(*train_ind)(labels)\n",
    "        test_labels = itemgetter(*test_ind)(labels)\n",
    "        \n",
    "        vocab = get_vocab_doc_freq(train_files)\n",
    "        \n",
    "        X_train = create_vector(train_files,voc)\n",
    "        Y_train = np.array(train_labels)\n",
    "        X_test = create_vector(test_files,voc)\n",
    "        Y_test = np.array(test_labels)\n",
    "        \n",
    "        for l in classes:\n",
    "            prior[l] = compute_prior(Y_train,l)\n",
    "            \n",
    "       \n",
    "        Y_predict = predict_y(X_train,Y_train,classes, X_test,prior,voc)\n",
    "        \n",
    "        \n",
    "        acc = accuracy_score(Y_test,Y_predict,normalize=True, sample_weight=None)\n",
    "        c_matrix = confusion_matrix(Y_test, Y_predict)\n",
    "        prec = precision_score(Y_test, Y_predict) \n",
    "        rec = recall_score(Y_test, Y_predict)  \n",
    "        fm = f1_score(Y_test, Y_predict)\n",
    "    \n",
    "        accuracy.append(acc)\n",
    "        recall.append(rec)\n",
    "        precision.append(prec)\n",
    "        f_measure.append(fm)\n",
    "        \n",
    "        if verbose:\n",
    "            print ('fold:', fold)\n",
    "            print ('accuracy:', acc)\n",
    "            print ('confusion_matrix')\n",
    "            print (c_matrix)\n",
    "            print ('prediction', prec)\n",
    "            print ('recall' , rec)\n",
    "            print ('f-measure',fm)\n",
    "            print ('')\n",
    "        fold += 1\n",
    "    \n",
    "    print (accuracy)\n",
    "    print (precision)\n",
    "    print (recall)\n",
    "    print (f_measure)\n",
    "    print (' ')\n",
    "    \n",
    "    avg_acc = sum(accuracy)/len(accuracy)\n",
    "    avg_pre = sum(precision)/len(precision)\n",
    "    avg_rec = sum(recall)/len(recall)\n",
    "    avg_fm = sum(f_measure)/len(f_measure)\n",
    "   \n",
    "    print ('avg_accuracy: ' , avg_acc)\n",
    "    print ('avg_precision', avg_pre)\n",
    "    print ('avg_recall', avg_rec)\n",
    "    print ('avg_fmeasure', avg_fm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam2 = tokenize(\"data/enron2/spam/*\",None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham2 = tokenize(\"data/enron2/ham/*\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making vocabulary\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-8c37515ff6e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vocab_doc_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspam2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mham2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-a73197761ad9>\u001b[0m in \u001b[0;36mget_vocab_doc_freq\u001b[0;34m(list_of_docs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_docs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mdoc_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab = get_vocab_doc_freq(spam2+ham2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "pickle_files(spam2,\"spam_tokens2\",load=False,dump=True)\n",
    "pickle_files(ham2,\"ham_tokens2\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_files(vocab,\"vocabulary_2\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam3 = tokenize(\"data/enron3/spam/*\",None)\n",
    "ham3 = tokenize(\"data/enron3/ham/*\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "pickle_files(spam3,\"spam_tokens3\",load=False,dump=True)\n",
    "pickle_files(ham3,\"ham_tokens3\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam4 = tokenize(\"data/enron4/spam/*\",None)\n",
    "ham4 = tokenize(\"data/enron4/ham/*\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam5 = tokenize(\"data/enron5/spam/*\",None)\n",
    "ham5 = tokenize(\"data/enron5/ham/*\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n",
      "1500\n",
      "3675\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print len(spam4)\n",
    "# spam4[:10]\n",
    "print len(ham4)\n",
    "print len(spam5)\n",
    "print len(ham5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n",
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "pickle_files(spam4,\"spam_tokens4\",load=False,dump=True)\n",
    "pickle_files(ham4,\"ham_tokens4\",load=False,dump=True)\n",
    "pickle_files(spam5,\"spam_tokens5\",load=False,dump=True)\n",
    "pickle_files(ham5,\"ham_tokens5\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam2_ = tokenize(\"data/enron2/spam/*\",None)\n",
    "ham2_ = tokenize(\"data/enron2/ham/*\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "# vocab2_ = get_vocab_doc_freq(spam2+ham2)\n",
    "# pickle_files(vocab2_,\"vocabulary2_\",load=False,dump=True)\n",
    "pickle_files(spam2_,\"spam_tokens2_\",load=False,dump=True)\n",
    "pickle_files(ham2_,\"ham_tokens2_\",load=False,dump=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "spam_ = tokenize(\"data/enron1/spam/*\",None)\n",
    "ham_ = tokenize(\"data/enron1/ham/*\",None)\n",
    "pickle_files(spam_,\"spam_tokens_\",load=False,dump=True)\n",
    "pickle_files(ham_,\"ham_tokens_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_ = get_vocab_doc_freq(spam_+ham_)\n",
    "pickle_files(voc_,\"voc/voc_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "spam2_ = tokenize(\"data/enron2/spam/*\",None)\n",
    "ham2_ = tokenize(\"data/enron2/ham/*\",None)\n",
    "pickle_files(spam2_,\"spam_tokens2_\",load=False,dump=True)\n",
    "pickle_files(ham2_,\"ham_tokens2_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1496"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "spam3_ = tokenize(\"data/enron3/spam/*\",None)\n",
    "ham3_ = tokenize(\"data/enron3/ham/*\",None)\n",
    "pickle_files(spam2_,\"spam_tokens3_\",load=False,dump=True)\n",
    "pickle_files(ham2_,\"ham_tokens3_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "spam4_ = tokenize(\"data/enron4/spam/*\",None)\n",
    "ham4_ = tokenize(\"data/enron4/ham/*\",None)\n",
    "pickle_files(spam2_,\"spam_tokens4_\",load=False,dump=True)\n",
    "pickle_files(ham2_,\"ham_tokens4_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print len(spam4_)\n",
    "print len(ham4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "spam5_ = tokenize(\"data/enron5/spam/*\",None)\n",
    "ham5_ = tokenize(\"data/enron5/ham/*\",None)\n",
    "pickle_files(spam2_,\"spam_tokens5_\",load=False,dump=True)\n",
    "pickle_files(ham2_,\"ham_tokens5_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc5_ = get_vocab_doc_freq(spam5_+ham5_)\n",
    "pickle_files(voc5_,\"voc/voc5_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print len(spam5_)\n",
    "print len(ham5_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of writing file\n",
      "end of writing file\n"
     ]
    }
   ],
   "source": [
    "spam6_ = tokenize(\"data/enron6/spam/*\",None)\n",
    "ham6_ = tokenize(\"data/enron6/ham/*\",None)\n",
    "pickle_files(spam6_,\"spam_tokens6_\",load=False,dump=True)\n",
    "pickle_files(ham6_,\"ham_tokens6_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc6_ = get_vocab_doc_freq(spam6_+ham6_)\n",
    "pickle_files(voc5_,\"voc/voc6_\",load=False,dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print len(spam6_)\n",
    "print len(ham6_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
